{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09b62808-d8ea-4f5a-b9f4-9b4711c2489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2084e7f7-7baa-42f4-bc5c-0f62c4d34b9e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Json file to CSV (Interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06bdc940-d422-4e66-afbb-74037591c569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 1 to Chunks\\part_1.csv\n",
      "Saved chunk 2 to Chunks\\part_2.csv\n",
      "Saved chunk 3 to Chunks\\part_3.csv\n",
      "Saved chunk 4 to Chunks\\part_4.csv\n",
      "Saved chunk 5 to Chunks\\part_5.csv\n",
      "Saved chunk 6 to Chunks\\part_6.csv\n",
      "Saved chunk 7 to Chunks\\part_7.csv\n",
      "Saved chunk 8 to Chunks\\part_8.csv\n",
      "Saved chunk 9 to Chunks\\part_9.csv\n",
      "Saved chunk 10 to Chunks\\part_10.csv\n",
      "Saved chunk 11 to Chunks\\part_11.csv\n",
      "Saved chunk 12 to Chunks\\part_12.csv\n",
      "Saved chunk 13 to Chunks\\part_13.csv\n",
      "Saved chunk 14 to Chunks\\part_14.csv\n",
      "Saved chunk 15 to Chunks\\part_15.csv\n",
      "Saved chunk 16 to Chunks\\part_16.csv\n",
      "Saved chunk 17 to Chunks\\part_17.csv\n",
      "Saved chunk 18 to Chunks\\part_18.csv\n",
      "Saved chunk 19 to Chunks\\part_19.csv\n",
      "Saved chunk 20 to Chunks\\part_20.csv\n",
      "Saved chunk 21 to Chunks\\part_21.csv\n",
      "Saved chunk 22 to Chunks\\part_22.csv\n",
      "Saved chunk 23 to Chunks\\part_23.csv\n",
      "Saved chunk 24 to Chunks\\part_24.csv\n",
      "Saved chunk 25 to Chunks\\part_25.csv\n",
      "Saved chunk 26 to Chunks\\part_26.csv\n",
      "Saved chunk 27 to Chunks\\part_27.csv\n",
      "Saved chunk 28 to Chunks\\part_28.csv\n",
      "Saved chunk 29 to Chunks\\part_29.csv\n",
      "Saved chunk 30 to Chunks\\part_30.csv\n",
      "Saved chunk 31 to Chunks\\part_31.csv\n",
      "Saved chunk 32 to Chunks\\part_32.csv\n",
      "Saved chunk 33 to Chunks\\part_33.csv\n",
      "Saved chunk 34 to Chunks\\part_34.csv\n",
      "Saved chunk 35 to Chunks\\part_35.csv\n",
      "Saved chunk 36 to Chunks\\part_36.csv\n",
      "Saved chunk 37 to Chunks\\part_37.csv\n",
      "Saved chunk 38 to Chunks\\part_38.csv\n",
      "Saved chunk 39 to Chunks\\part_39.csv\n",
      "Saved chunk 40 to Chunks\\part_40.csv\n",
      "Saved chunk 41 to Chunks\\part_41.csv\n",
      "Saved chunk 42 to Chunks\\part_42.csv\n",
      "Saved chunk 43 to Chunks\\part_43.csv\n",
      "Saved chunk 44 to Chunks\\part_44.csv\n",
      "Saved chunk 45 to Chunks\\part_45.csv\n",
      "Saved chunk 46 to Chunks\\part_46.csv\n",
      "Saved chunk 47 to Chunks\\part_47.csv\n",
      "Saved chunk 48 to Chunks\\part_48.csv\n",
      "Saved chunk 49 to Chunks\\part_49.csv\n",
      "Saved chunk 50 to Chunks\\part_50.csv\n",
      "Saved chunk 51 to Chunks\\part_51.csv\n",
      "Saved chunk 52 to Chunks\\part_52.csv\n",
      "Saved chunk 53 to Chunks\\part_53.csv\n",
      "Saved chunk 54 to Chunks\\part_54.csv\n",
      "Saved chunk 55 to Chunks\\part_55.csv\n",
      "Saved chunk 56 to Chunks\\part_56.csv\n",
      "Saved chunk 57 to Chunks\\part_57.csv\n",
      "Saved chunk 58 to Chunks\\part_58.csv\n",
      "Saved chunk 59 to Chunks\\part_59.csv\n",
      "Saved chunk 60 to Chunks\\part_60.csv\n",
      "Saved chunk 61 to Chunks\\part_61.csv\n",
      "Saved chunk 62 to Chunks\\part_62.csv\n",
      "Saved chunk 63 to Chunks\\part_63.csv\n",
      "Saved chunk 64 to Chunks\\part_64.csv\n",
      "Saved chunk 65 to Chunks\\part_65.csv\n",
      "Saved chunk 66 to Chunks\\part_66.csv\n",
      "Saved chunk 67 to Chunks\\part_67.csv\n",
      "Saved chunk 68 to Chunks\\part_68.csv\n",
      "Saved chunk 69 to Chunks\\part_69.csv\n",
      "Saved chunk 70 to Chunks\\part_70.csv\n",
      "Saved chunk 71 to Chunks\\part_71.csv\n",
      "Saved chunk 72 to Chunks\\part_72.csv\n",
      "Saved chunk 73 to Chunks\\part_73.csv\n",
      "Saved chunk 74 to Chunks\\part_74.csv\n",
      "Saved chunk 75 to Chunks\\part_75.csv\n",
      "Saved chunk 76 to Chunks\\part_76.csv\n",
      "Saved chunk 77 to Chunks\\part_77.csv\n",
      "Saved chunk 78 to Chunks\\part_78.csv\n",
      "Saved chunk 79 to Chunks\\part_79.csv\n",
      "Saved chunk 80 to Chunks\\part_80.csv\n",
      "Saved chunk 81 to Chunks\\part_81.csv\n",
      "Saved chunk 82 to Chunks\\part_82.csv\n",
      "Saved chunk 83 to Chunks\\part_83.csv\n",
      "Saved chunk 84 to Chunks\\part_84.csv\n",
      "Saved chunk 85 to Chunks\\part_85.csv\n",
      "Saved chunk 86 to Chunks\\part_86.csv\n",
      "Saved chunk 87 to Chunks\\part_87.csv\n",
      "Saved chunk 88 to Chunks\\part_88.csv\n",
      "Saved chunk 89 to Chunks\\part_89.csv\n",
      "Saved chunk 90 to Chunks\\part_90.csv\n",
      "Saved chunk 91 to Chunks\\part_91.csv\n",
      "Saved chunk 92 to Chunks\\part_92.csv\n",
      "Saved chunk 93 to Chunks\\part_93.csv\n",
      "Saved chunk 94 to Chunks\\part_94.csv\n",
      "Saved chunk 95 to Chunks\\part_95.csv\n",
      "Saved chunk 96 to Chunks\\part_96.csv\n",
      "Saved chunk 97 to Chunks\\part_97.csv\n",
      "Saved chunk 98 to Chunks\\part_98.csv\n",
      "Saved chunk 99 to Chunks\\part_99.csv\n",
      "Saved chunk 100 to Chunks\\part_100.csv\n",
      "Saved chunk 101 to Chunks\\part_101.csv\n",
      "Saved chunk 102 to Chunks\\part_102.csv\n",
      "Saved chunk 103 to Chunks\\part_103.csv\n",
      "Saved chunk 104 to Chunks\\part_104.csv\n",
      "Saved chunk 105 to Chunks\\part_105.csv\n",
      "Saved chunk 106 to Chunks\\part_106.csv\n",
      "Saved chunk 107 to Chunks\\part_107.csv\n",
      "Saved chunk 108 to Chunks\\part_108.csv\n",
      "Saved chunk 109 to Chunks\\part_109.csv\n",
      "Saved chunk 110 to Chunks\\part_110.csv\n",
      "Saved chunk 111 to Chunks\\part_111.csv\n",
      "Saved chunk 112 to Chunks\\part_112.csv\n",
      "Saved chunk 113 to Chunks\\part_113.csv\n",
      "Saved chunk 114 to Chunks\\part_114.csv\n",
      "Saved chunk 115 to Chunks\\part_115.csv\n",
      "Saved chunk 116 to Chunks\\part_116.csv\n",
      "Saved chunk 117 to Chunks\\part_117.csv\n",
      "Saved chunk 118 to Chunks\\part_118.csv\n",
      "Saved chunk 119 to Chunks\\part_119.csv\n",
      "Saved chunk 120 to Chunks\\part_120.csv\n",
      "Saved chunk 121 to Chunks\\part_121.csv\n",
      "Saved chunk 122 to Chunks\\part_122.csv\n",
      "Saved chunk 123 to Chunks\\part_123.csv\n",
      "Saved chunk 124 to Chunks\\part_124.csv\n",
      "Saved chunk 125 to Chunks\\part_125.csv\n",
      "Saved chunk 126 to Chunks\\part_126.csv\n",
      "Saved chunk 127 to Chunks\\part_127.csv\n",
      "Saved chunk 128 to Chunks\\part_128.csv\n",
      "Saved chunk 129 to Chunks\\part_129.csv\n",
      "Saved chunk 130 to Chunks\\part_130.csv\n",
      "Saved chunk 131 to Chunks\\part_131.csv\n",
      "Saved chunk 132 to Chunks\\part_132.csv\n",
      "Saved chunk 133 to Chunks\\part_133.csv\n",
      "Saved chunk 134 to Chunks\\part_134.csv\n",
      "Saved chunk 135 to Chunks\\part_135.csv\n",
      "Saved chunk 136 to Chunks\\part_136.csv\n",
      "Saved chunk 137 to Chunks\\part_137.csv\n",
      "Saved chunk 138 to Chunks\\part_138.csv\n",
      "Saved chunk 139 to Chunks\\part_139.csv\n",
      "Saved chunk 140 to Chunks\\part_140.csv\n",
      "Saved chunk 141 to Chunks\\part_141.csv\n",
      "Saved chunk 142 to Chunks\\part_142.csv\n",
      "Saved chunk 143 to Chunks\\part_143.csv\n",
      "Saved chunk 144 to Chunks\\part_144.csv\n",
      "Saved chunk 145 to Chunks\\part_145.csv\n",
      "Saved chunk 146 to Chunks\\part_146.csv\n",
      "Saved chunk 147 to Chunks\\part_147.csv\n",
      "Saved chunk 148 to Chunks\\part_148.csv\n",
      "Saved chunk 149 to Chunks\\part_149.csv\n",
      "Saved chunk 150 to Chunks\\part_150.csv\n",
      "Saved chunk 151 to Chunks\\part_151.csv\n",
      "Saved chunk 152 to Chunks\\part_152.csv\n",
      "Saved chunk 153 to Chunks\\part_153.csv\n",
      "Saved chunk 154 to Chunks\\part_154.csv\n",
      "Saved chunk 155 to Chunks\\part_155.csv\n",
      "Saved chunk 156 to Chunks\\part_156.csv\n",
      "Saved chunk 157 to Chunks\\part_157.csv\n",
      "Saved chunk 158 to Chunks\\part_158.csv\n",
      "Saved chunk 159 to Chunks\\part_159.csv\n",
      "Saved chunk 160 to Chunks\\part_160.csv\n",
      "Saved chunk 161 to Chunks\\part_161.csv\n",
      "Saved chunk 162 to Chunks\\part_162.csv\n",
      "Saved chunk 163 to Chunks\\part_163.csv\n",
      "Saved chunk 164 to Chunks\\part_164.csv\n",
      "Saved chunk 165 to Chunks\\part_165.csv\n",
      "Saved chunk 166 to Chunks\\part_166.csv\n",
      "Saved chunk 167 to Chunks\\part_167.csv\n",
      "Saved chunk 168 to Chunks\\part_168.csv\n",
      "Saved chunk 169 to Chunks\\part_169.csv\n",
      "Saved chunk 170 to Chunks\\part_170.csv\n",
      "Saved chunk 171 to Chunks\\part_171.csv\n",
      "Saved chunk 172 to Chunks\\part_172.csv\n",
      "Saved chunk 173 to Chunks\\part_173.csv\n",
      "Saved chunk 174 to Chunks\\part_174.csv\n",
      "Saved chunk 175 to Chunks\\part_175.csv\n",
      "Saved chunk 176 to Chunks\\part_176.csv\n",
      "Saved chunk 177 to Chunks\\part_177.csv\n",
      "Saved chunk 178 to Chunks\\part_178.csv\n",
      "Saved chunk 179 to Chunks\\part_179.csv\n",
      "Saved chunk 180 to Chunks\\part_180.csv\n",
      "Saved chunk 181 to Chunks\\part_181.csv\n",
      "Saved chunk 182 to Chunks\\part_182.csv\n",
      "Saved chunk 183 to Chunks\\part_183.csv\n",
      "Saved chunk 184 to Chunks\\part_184.csv\n",
      "Saved chunk 185 to Chunks\\part_185.csv\n",
      "Saved chunk 186 to Chunks\\part_186.csv\n",
      "Saved chunk 187 to Chunks\\part_187.csv\n",
      "Saved chunk 188 to Chunks\\part_188.csv\n",
      "Saved chunk 189 to Chunks\\part_189.csv\n",
      "Saved chunk 190 to Chunks\\part_190.csv\n",
      "Saved chunk 191 to Chunks\\part_191.csv\n",
      "Saved chunk 192 to Chunks\\part_192.csv\n",
      "Saved chunk 193 to Chunks\\part_193.csv\n",
      "Saved chunk 194 to Chunks\\part_194.csv\n",
      "Saved chunk 195 to Chunks\\part_195.csv\n",
      "Saved chunk 196 to Chunks\\part_196.csv\n",
      "Saved chunk 197 to Chunks\\part_197.csv\n",
      "Saved chunk 198 to Chunks\\part_198.csv\n",
      "Saved chunk 199 to Chunks\\part_199.csv\n",
      "Saved chunk 200 to Chunks\\part_200.csv\n",
      "Saved chunk 201 to Chunks\\part_201.csv\n",
      "Saved final chunk 202 to Chunks\\part_202.csv\n",
      "Final merged CSV saved to interactions_data_file.csv\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "json_file_path = \"goodreads_interactions_children.json\"\n",
    "output_folder = \"Chunks\"\n",
    "final_csv_path = \"interactions_data_file.csv\"\n",
    "\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Process large JSON file in chunks\n",
    "chunk_size = 50000  # Number of records per chunk\n",
    "chunk_count = 0\n",
    "header_written = False\n",
    "part_files = []  # List to store chunk file names\n",
    "\n",
    "with open(json_file_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "    data_buffer = []\n",
    "    for line in json_file:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            try:\n",
    "                data_buffer.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Skipping invalid line: {line}\")\n",
    "\n",
    "        # When buffer reaches chunk size, write to CSV\n",
    "        if len(data_buffer) >= chunk_size:\n",
    "            chunk_count += 1\n",
    "            part_file = os.path.join(output_folder, f\"part_{chunk_count}.csv\")\n",
    "            part_files.append(part_file)\n",
    "\n",
    "            # Write data to CSV\n",
    "            try:\n",
    "                with open(part_file, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "                    writer = csv.writer(csv_file)\n",
    "                    if not header_written:\n",
    "                        writer.writerow(data_buffer[0].keys())  # Write header once\n",
    "                        header_written = True\n",
    "\n",
    "                    for item in data_buffer:\n",
    "                        writer.writerow(item.values())\n",
    "\n",
    "                print(f\"Saved chunk {chunk_count} to {part_file}\")\n",
    "\n",
    "            finally:\n",
    "                data_buffer.clear()  # Clear buffer only after successful write\n",
    "                header_written = False; # let the next chunk file has the header\n",
    "\n",
    "# Save remaining data in the buffer (last chunk)\n",
    "if data_buffer:\n",
    "    chunk_count += 1\n",
    "    part_file = os.path.join(output_folder, f\"part_{chunk_count}.csv\")\n",
    "    part_files.append(part_file)\n",
    "\n",
    "    with open(part_file, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(data_buffer[0].keys())\n",
    "        for item in data_buffer:\n",
    "            writer.writerow(item.values())\n",
    "\n",
    "    print(f\"Saved final chunk {chunk_count} to {part_file}\")\n",
    "\n",
    "# Merge all CSV chunks into one final CSV file\n",
    "with open(final_csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as final_csv:\n",
    "    writer = csv.writer(final_csv)\n",
    "    header_written = False\n",
    "\n",
    "    for part_file in part_files:\n",
    "        with open(part_file, \"r\", encoding=\"utf-8\") as part_csv:\n",
    "            reader = csv.reader(part_csv)\n",
    "            header = next(reader)\n",
    "            if not header_written:\n",
    "                writer.writerow(header)  # Write header only once\n",
    "                header_written = True\n",
    "            for row in reader:\n",
    "                writer.writerow(row)\n",
    "\n",
    "print(f\"Final merged CSV saved to {final_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8781afcb-5ce4-4dab-a713-06185bc3ca3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 10059349\n"
     ]
    }
   ],
   "source": [
    "# Define the file path\n",
    "csv_file_path = \"interactions_data_file.csv\"  # Update with your actual path\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Count the total number of rows\n",
    "total_rows = len(df)\n",
    "\n",
    "print(f\"Total number of rows: {total_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e7584c-d5ed-4098-9ddd-f6b74e423667",
   "metadata": {},
   "source": [
    "### Json File to CSV (Reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6122b3c4-4a22-43b0-a219-5ff976be96bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 1 to Chunks_Reviews\\part_1.csv\n",
      "Saved chunk 2 to Chunks_Reviews\\part_2.csv\n",
      "Saved chunk 3 to Chunks_Reviews\\part_3.csv\n",
      "Saved chunk 4 to Chunks_Reviews\\part_4.csv\n",
      "Saved chunk 5 to Chunks_Reviews\\part_5.csv\n",
      "Saved chunk 6 to Chunks_Reviews\\part_6.csv\n",
      "Saved chunk 7 to Chunks_Reviews\\part_7.csv\n",
      "Saved chunk 8 to Chunks_Reviews\\part_8.csv\n",
      "Saved chunk 9 to Chunks_Reviews\\part_9.csv\n",
      "Saved chunk 10 to Chunks_Reviews\\part_10.csv\n",
      "Saved chunk 11 to Chunks_Reviews\\part_11.csv\n",
      "Saved chunk 12 to Chunks_Reviews\\part_12.csv\n",
      "Saved chunk 13 to Chunks_Reviews\\part_13.csv\n",
      "Saved chunk 14 to Chunks_Reviews\\part_14.csv\n",
      "Saved final chunk 15 to Chunks_Reviews\\part_15.csv\n",
      "Final merged CSV saved to reviews_data_file.csv\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "json_file_path = \"goodreads_reviews_children.json\"\n",
    "output_folder = \"Chunks_Reviews\"\n",
    "final_csv_path = \"reviews_data_file.csv\"\n",
    "\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Process large JSON file in chunks\n",
    "chunk_size = 50000  # Number of records per chunk\n",
    "chunk_count = 0\n",
    "header_written = False\n",
    "part_files = []  # List to store chunk file names\n",
    "\n",
    "with open(json_file_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "    data_buffer = []\n",
    "    for line in json_file:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            try:\n",
    "                data_buffer.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Skipping invalid line: {line}\")\n",
    "\n",
    "        # When buffer reaches chunk size, write to CSV\n",
    "        if len(data_buffer) >= chunk_size:\n",
    "            chunk_count += 1\n",
    "            part_file = os.path.join(output_folder, f\"part_{chunk_count}.csv\")\n",
    "            part_files.append(part_file)\n",
    "\n",
    "            # Write data to CSV\n",
    "            try:\n",
    "                with open(part_file, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "                    writer = csv.writer(csv_file)\n",
    "                    if not header_written:\n",
    "                        writer.writerow(data_buffer[0].keys())  # Write header once\n",
    "                        header_written = True\n",
    "\n",
    "                    for item in data_buffer:\n",
    "                        writer.writerow(item.values())\n",
    "\n",
    "                print(f\"Saved chunk {chunk_count} to {part_file}\")\n",
    "\n",
    "            finally:\n",
    "                data_buffer.clear()  # Clear buffer only after successful write\n",
    "                header_written = False; # let the next chunk file has the header\n",
    "\n",
    "# Save remaining data in the buffer (last chunk)\n",
    "if data_buffer:\n",
    "    chunk_count += 1\n",
    "    part_file = os.path.join(output_folder, f\"part_{chunk_count}.csv\")\n",
    "    part_files.append(part_file)\n",
    "\n",
    "    with open(part_file, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(data_buffer[0].keys())\n",
    "        for item in data_buffer:\n",
    "            writer.writerow(item.values())\n",
    "\n",
    "    print(f\"Saved final chunk {chunk_count} to {part_file}\")\n",
    "\n",
    "# Merge all CSV chunks into one final CSV file\n",
    "with open(final_csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as final_csv:\n",
    "    writer = csv.writer(final_csv)\n",
    "    header_written = False\n",
    "\n",
    "    for part_file in part_files:\n",
    "        with open(part_file, \"r\", encoding=\"utf-8\") as part_csv:\n",
    "            reader = csv.reader(part_csv)\n",
    "            header = next(reader)\n",
    "            if not header_written:\n",
    "                writer.writerow(header)  # Write header only once\n",
    "                header_written = True\n",
    "            for row in reader:\n",
    "                writer.writerow(row)\n",
    "\n",
    "print(f\"Final merged CSV saved to {final_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d71edbb3-e97b-4553-b211-aa77bf11eea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 734640\n"
     ]
    }
   ],
   "source": [
    "# Define the file path\n",
    "csv_file_path = \"reviews_data_file.csv\"  # Update with your actual path\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Count the total number of rows\n",
    "total_rows = len(df)\n",
    "\n",
    "print(f\"Total number of rows: {total_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716b5d61-89ef-42c1-a065-fca862a6a90f",
   "metadata": {},
   "source": [
    "### Json file to CSV (Books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37f5b536-aa08-457c-a7de-b9bd48beae97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 1 to Chunks_Books\\part_1.csv\n",
      "Saved chunk 2 to Chunks_Books\\part_2.csv\n",
      "Saved final chunk 3 to Chunks_Books\\part_3.csv\n",
      "Final merged CSV saved to books_data_file.csv\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "json_file_path = \"goodreads_books_children.json\"\n",
    "output_folder = \"Chunks_Books\"\n",
    "final_csv_path = \"books_data_file.csv\"\n",
    "\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Process large JSON file in chunks\n",
    "chunk_size = 50000  # Number of records per chunk\n",
    "chunk_count = 0\n",
    "header_written = False\n",
    "part_files = []  # List to store chunk file names\n",
    "\n",
    "with open(json_file_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "    data_buffer = []\n",
    "    for line in json_file:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            try:\n",
    "                data_buffer.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Skipping invalid line: {line}\")\n",
    "\n",
    "        # When buffer reaches chunk size, write to CSV\n",
    "        if len(data_buffer) >= chunk_size:\n",
    "            chunk_count += 1\n",
    "            part_file = os.path.join(output_folder, f\"part_{chunk_count}.csv\")\n",
    "            part_files.append(part_file)\n",
    "\n",
    "            # Write data to CSV\n",
    "            try:\n",
    "                with open(part_file, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "                    writer = csv.writer(csv_file)\n",
    "                    if not header_written:\n",
    "                        writer.writerow(data_buffer[0].keys())  # Write header once\n",
    "                        header_written = True\n",
    "\n",
    "                    for item in data_buffer:\n",
    "                        writer.writerow(item.values())\n",
    "\n",
    "                print(f\"Saved chunk {chunk_count} to {part_file}\")\n",
    "\n",
    "            finally:\n",
    "                data_buffer.clear()  # Clear buffer only after successful write\n",
    "                header_written = False; # let the next chunk file has the header\n",
    "\n",
    "# Save remaining data in the buffer (last chunk)\n",
    "if data_buffer:\n",
    "    chunk_count += 1\n",
    "    part_file = os.path.join(output_folder, f\"part_{chunk_count}.csv\")\n",
    "    part_files.append(part_file)\n",
    "\n",
    "    with open(part_file, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(data_buffer[0].keys())\n",
    "        for item in data_buffer:\n",
    "            writer.writerow(item.values())\n",
    "\n",
    "    print(f\"Saved final chunk {chunk_count} to {part_file}\")\n",
    "\n",
    "# Merge all CSV chunks into one final CSV file\n",
    "with open(final_csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as final_csv:\n",
    "    writer = csv.writer(final_csv)\n",
    "    header_written = False\n",
    "\n",
    "    for part_file in part_files:\n",
    "        with open(part_file, \"r\", encoding=\"utf-8\") as part_csv:\n",
    "            reader = csv.reader(part_csv)\n",
    "            header = next(reader)\n",
    "            if not header_written:\n",
    "                writer.writerow(header)  # Write header only once\n",
    "                header_written = True\n",
    "            for row in reader:\n",
    "                writer.writerow(row)\n",
    "\n",
    "print(f\"Final merged CSV saved to {final_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ae6adb7-27b7-4b3e-b338-3c78b2132f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 124082\n"
     ]
    }
   ],
   "source": [
    "# Define the file path\n",
    "csv_file_path = \"books_data_file.csv\"  # Update with your actual path\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Count the total number of rows\n",
    "total_rows = len(df)\n",
    "\n",
    "print(f\"Total number of rows: {total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b47954d-873b-4a64-8ba7-89c013ddf75a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
