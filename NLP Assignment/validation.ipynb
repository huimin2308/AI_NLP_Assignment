{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "244fea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fec7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sentence transformer\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "427a47bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_answer = \"\"\"The document contains Quiz 9 from a Machine Learning course (BMCS2114) with five multiple-choice questions focused on reinforcement learning:\n",
    "Q1 asks about characteristics of reinforcement learning, listing options like sequential decision-making (c) and environment-driven data (d).\n",
    "Q2 explores feedback mechanisms, highlighting delayed feedback linked to the agent’s actions (c).\n",
    "Q3 distinguishes between reinforcement learning types, contrasting model-based (a) and model-free (b) approaches.\n",
    "Q4 addresses the objective, with the goal being to determine an optimal policy (c).\n",
    "Q5 clarifies what the agent receives post-action: a scalar reward (c).\n",
    "The content strictly outlines conceptual themes in reinforcement learning without indicating correct answers.\"\"\"\n",
    "\n",
    "reference_answer = \"\"\"The document titled \"Quiz 9\" from the course BMCS2114 Machine Learning contains a set of five multiple-choice questions focused on Reinforcement Learning (RL). The quiz assesses understanding of key RL concepts, including:\n",
    "1. Characteristics of RL, emphasizing sequential decision-making.\n",
    "2. Nature of feedback, highlighting that it is delayed and based on agent actions.\n",
    "3. Model-free vs. model-based approaches, where model-free RL does not rely on an environmental model.\n",
    "4. Objective of RL, which is to determine the optimal policy.\n",
    "5. Agent-environment interaction, specifically that an agent receives a scalar reward after an action.\n",
    "Would you like the correct answers or explanations for each question?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "351640d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity\n",
    "emb1 = model.encode(chatbot_answer, convert_to_tensor=True)\n",
    "emb2 = model.encode(reference_answer, convert_to_tensor=True)\n",
    "cosine_similarity = util.cos_sim(emb1, emb2).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee976663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.7614\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cosine Similarity: {cosine_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fe5ea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_answer = \"\"\"BMCS2114 机器学习 测验 9  Q1. 强化学习的特征是什么？ a) 即时反馈 b) 存在监督者 c) 顺序决策 d) 环境决定后续数据  Q2. 强化学习的反馈机制如何运作？ a) 反馈即时且立即 b) 由监督者提供反馈 c) 反馈延迟，基于智能体的行为 d) 由环境决定反馈  Q3. 哪种强化学习类型不依赖模型解决问题？ a) 基于模型的强化学习 b) 无模型的强化学习 c) 价值函数驱动的强化学习 d) 策略驱动的强化学习  Q4. 强化学习的目标是什么？ a) 最大化即时奖励 b) 最小化长期回报 c) 确定智能体的最优策略 d) 模仿环境的行为  Q5. 智能体执行动作后从环境中获得什么？ a) 环境的当前状态 b) 下一步动作的策略 c) 标量奖励或强化信号 d) 环境模型  关键术语对照： 强化学习 (Reinforcement Learning) 顺序决策 (Sequential Decision Making) 无模型的强化学习 (Model-Free RL) 最优策略 (Optimal Policy) 标量奖励 (Scalar Reward) 翻译严格遵循原文结构与技术术语，未添加额外内容。\"\"\"\n",
    "\n",
    "reference_answer = \"\"\"该文档是 BMCS2114 机器学习 课程的 第9次测验（Quiz 9），内容主要围绕 强化学习（Reinforcement Learning, RL） 的核心概念展开。测验包括五道多项选择题，主要考察以下知识点：  强化学习的特点，强调其是一个 序列决策过程。  反馈的性质，指出其是 延迟的，并取决于智能体的行为。  是否使用模型的问题，指出 无模型强化学习（Model-free RL） 不依赖环境模型。  强化学习的目标，是 确定最优策略（optimal policy）。  智能体与环境的交互，智能体在执行动作后会获得一个 标量奖励（scalar reward）。  需要我把整份测验逐题翻译成中文吗？\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df46edc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.6207\n"
     ]
    }
   ],
   "source": [
    "# Compute cosine similarity\n",
    "emb1 = model.encode(chatbot_answer, convert_to_tensor=True)\n",
    "emb2 = model.encode(reference_answer, convert_to_tensor=True)\n",
    "cosine_similarity = util.cos_sim(emb1, emb2).item()\n",
    "print(f\"Cosine Similarity: {cosine_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bf6de40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.7432\n"
     ]
    }
   ],
   "source": [
    "chatbot_answer = \"\"\"Summary of Quiz 9 on Reinforcement Learning (BMCS2114):\n",
    "Q1: Reinforcement learning is characterized by sequential decision making (c).\n",
    "Q2: Feedback in reinforcement learning is delayed and based on the agent's actions (c).\n",
    "Q3: Model-free RL does not use a model to solve problems (b).\n",
    "Q4: The objective is to determine the optimal policy for the agent (c).\n",
    "Q5: After performing an action, the agent receives a scalar reward or reinforcement from the environment (c).\n",
    "Answers are derived directly from the quiz content and standard reinforcement learning principles.\"\"\"\n",
    "\n",
    "reference_answer = \"\"\"The document titled \"Quiz 9\" from the course BMCS2114 Machine Learning contains a set of five multiple-choice questions focused on Reinforcement Learning (RL). The quiz assesses understanding of key RL concepts, including:\n",
    "1. Characteristics of RL, emphasizing sequential decision-making.\n",
    "2. Nature of feedback, highlighting that it is delayed and based on agent actions.\n",
    "3. Model-free vs. model-based approaches, where model-free RL does not rely on an environmental model.\n",
    "4. Objective of RL, which is to determine the optimal policy.\n",
    "5. Agent-environment interaction, specifically that an agent receives a scalar reward after an action.\n",
    "Would you like the correct answers or explanations for each question?\n",
    "\"\"\"\n",
    "# Compute cosine similarity\n",
    "emb1 = model.encode(chatbot_answer, convert_to_tensor=True)\n",
    "emb2 = model.encode(reference_answer, convert_to_tensor=True)\n",
    "cosine_similarity = util.cos_sim(emb1, emb2).item()\n",
    "print(f\"Cosine Similarity: {cosine_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e5791d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.6457\n"
     ]
    }
   ],
   "source": [
    "chatbot_answer = \"\"\"以下是文档内容的简体中文翻译：  BMCS2114 机器学习 测验 9  问题 1：以下哪项是强化学习的特点？ a) 即时反馈 b) 存在监督者 c) 序列化决策 d) 环境决定后续数据  问题 2：强化学习中的反馈如何产生？ a) 反馈是即时且直接的 b) 由监督者提供反馈 c) 反馈延迟，基于智能体的行为 d) 由环境决定反馈  问题 3：哪种强化学习不需要使用模型解决问题？ a) 基于模型的强化学习 b) 无模型强化学习 c) 价值函数强化学习 d) 策略强化学习  问题 4：强化学习的目标是什么？ a) 最大化即时奖励 b) 最小化长期回报 c) 确定智能体的最优策略 d) 模仿环境的行为  问题 5：智能体执行动作后从环境中获得什么？ a) 环境的当前状态 b) 下一步动作的策略 c) 标量奖励或强化信号 d) 环境模型  翻译严格遵循原文内容，未添加额外信息，术语与结构均与原始文档一致。\"\"\"\n",
    "\n",
    "reference_answer = \"\"\"该文档是 BMCS2114 机器学习 课程的 第9次测验（Quiz 9），内容主要围绕 强化学习（Reinforcement Learning, RL） 的核心概念展开。测验包括五道多项选择题，主要考察以下知识点：  强化学习的特点，强调其是一个 序列决策过程。  反馈的性质，指出其是 延迟的，并取决于智能体的行为。  是否使用模型的问题，指出 无模型强化学习（Model-free RL） 不依赖环境模型。  强化学习的目标，是 确定最优策略（optimal policy）。  智能体与环境的交互，智能体在执行动作后会获得一个 标量奖励（scalar reward）。  需要我把整份测验逐题翻译成中文吗？\"\"\"\n",
    "\n",
    "# Compute cosine similarity\n",
    "emb1 = model.encode(chatbot_answer, convert_to_tensor=True)\n",
    "emb2 = model.encode(reference_answer, convert_to_tensor=True)\n",
    "cosine_similarity = util.cos_sim(emb1, emb2).item()\n",
    "print(f\"Cosine Similarity: {cosine_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fc3e222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.7812\n"
     ]
    }
   ],
   "source": [
    "chatbot_answer = \"\"\"Here is a summary of the content from the document:\n",
    "The document is a quiz (Quiz 9) related to reinforcement learning in the course \"BMCS2114 Machine Learning.\" It consists of five multiple-choice questions covering key concepts in reinforcement learning, including:\n",
    "1. Characteristics of reinforcement learning (e.g., sequential decision-making, delayed feedback).\n",
    "2. How feedback occurs in reinforcement learning (e.g., delayed and based on the agent's actions).\n",
    "3. Types of reinforcement learning (e.g., model-free RL).\n",
    "4. The objective of reinforcement learning (e.g., finding the optimal policy for the agent).\n",
    "5. What the agent receives from the environment after performing an action (e.g., a scalar reward).\n",
    "The questions focus on fundamental aspects of reinforcement learning, such as feedback mechanisms, model types, and the agent-environment interaction.\"\"\"\n",
    "\n",
    "reference_answer = \"\"\"The document titled \"Quiz 9\" from the course BMCS2114 Machine Learning contains a set of five multiple-choice questions focused on Reinforcement Learning (RL). The quiz assesses understanding of key RL concepts, including:\n",
    "1. Characteristics of RL, emphasizing sequential decision-making.\n",
    "2. Nature of feedback, highlighting that it is delayed and based on agent actions.\n",
    "3. Model-free vs. model-based approaches, where model-free RL does not rely on an environmental model.\n",
    "4. Objective of RL, which is to determine the optimal policy.\n",
    "5. Agent-environment interaction, specifically that an agent receives a scalar reward after an action.\n",
    "Would you like the correct answers or explanations for each question?\n",
    "\"\"\"\n",
    "# Compute cosine similarity\n",
    "emb1 = model.encode(chatbot_answer, convert_to_tensor=True)\n",
    "emb2 = model.encode(reference_answer, convert_to_tensor=True)\n",
    "cosine_similarity = util.cos_sim(emb1, emb2).item()\n",
    "print(f\"Cosine Similarity: {cosine_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24a52792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.6435\n"
     ]
    }
   ],
   "source": [
    "chatbot_answer = \"\"\"BMCS2114 机器学习  测验9  Q1. 以下哪项是强化学习的特点？ a) 即时反馈 b) 存在监督者 c) 序列决策 d) 环境决定后续数据  Q2. 强化学习中的反馈是如何发生的？ a) 即时且立即 b) 由监督者提供 c) 延迟且基于智能体的动作 d) 由环境决定  Q3. 哪种强化学习不需要模型来解决问题？ a) 基于模型的强化学习 b) 无模型强化学习 c) 价值函数强化学习 d) 策略强化学习  Q4. 强化学习的目标是什么？ a) 最大化即时奖励 b) 最小化长期回报 c) 确定智能体的最优策略 d) 模仿环境的行为  Q5. 智能体在执行动作后从环境中获得什么？ a) 环境的当前状态 b) 下一个动作的策略 c) 标量奖励或强化信号 d) 环境模型\"\"\"\n",
    "\n",
    "reference_answer = \"\"\"该文档是 BMCS2114 机器学习 课程的 第9次测验（Quiz 9），内容主要围绕 强化学习（Reinforcement Learning, RL） 的核心概念展开。测验包括五道多项选择题，主要考察以下知识点：  强化学习的特点，强调其是一个 序列决策过程。  反馈的性质，指出其是 延迟的，并取决于智能体的行为。  是否使用模型的问题，指出 无模型强化学习（Model-free RL） 不依赖环境模型。  强化学习的目标，是 确定最优策略（optimal policy）。  智能体与环境的交互，智能体在执行动作后会获得一个 标量奖励（scalar reward）。  需要我把整份测验逐题翻译成中文吗？\"\"\"\n",
    "\n",
    "# Compute cosine similarity\n",
    "emb1 = model.encode(chatbot_answer, convert_to_tensor=True)\n",
    "emb2 = model.encode(reference_answer, convert_to_tensor=True)\n",
    "cosine_similarity = util.cos_sim(emb1, emb2).item()\n",
    "print(f\"Cosine Similarity: {cosine_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f0d71a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.8314\n"
     ]
    }
   ],
   "source": [
    "chatbot_answer = \"\"\"The document is a quiz (Quiz 9) on BMCS2114 Machine Learning, focusing on reinforcement learning (RL). It consists of five multiple-choice questions covering key RL concepts:\n",
    "Q1: Highlights characteristics of RL, with the correct answer being sequential decision making.\n",
    "Q2: Explains RL feedback as delayed and based on the agent's actions.\n",
    "Q3: Identifies model-free RL as the type that does not use a model.\n",
    "Q4: States the objective is to determine the optimal policy for the agent.\n",
    "Q5: Describes the agent receiving a scalar reward from the environment after an action.\n",
    "The quiz tests understanding of RL fundamentals, including feedback mechanisms, model types, and goals.\n",
    "(Note: Summary strictly adheres to the provided document content.)\"\"\"\n",
    "\n",
    "reference_answer = \"\"\"The document titled \"Quiz 9\" from the course BMCS2114 Machine Learning contains a set of five multiple-choice questions focused on Reinforcement Learning (RL). The quiz assesses understanding of key RL concepts, including:\n",
    "1. Characteristics of RL, emphasizing sequential decision-making.\n",
    "2. Nature of feedback, highlighting that it is delayed and based on agent actions.\n",
    "3. Model-free vs. model-based approaches, where model-free RL does not rely on an environmental model.\n",
    "4. Objective of RL, which is to determine the optimal policy.\n",
    "5. Agent-environment interaction, specifically that an agent receives a scalar reward after an action.\n",
    "Would you like the correct answers or explanations for each question?\n",
    "\"\"\"\n",
    "# Compute cosine similarity\n",
    "emb1 = model.encode(chatbot_answer, convert_to_tensor=True)\n",
    "emb2 = model.encode(reference_answer, convert_to_tensor=True)\n",
    "cosine_similarity = util.cos_sim(emb1, emb2).item()\n",
    "print(f\"Cosine Similarity: {cosine_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33449474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.6583\n"
     ]
    }
   ],
   "source": [
    "chatbot_answer = \"\"\"BMCS2114 机器学习  测验9  Q1. 以下哪项是强化学习的特点？ a) 即时反馈 b) 存在监督者 c) 序列决策 d) 环境决定后续数据  Q2. 强化学习中的反馈如何发生？ a) 即时且立即 b) 由监督者提供 c) 延迟且基于智能体的行为 d) 由环境决定  Q3. 哪种强化学习不依赖模型解决问题？ a) 基于模型的强化学习 b) 无模型强化学习 c) 价值函数强化学习 d) 策略强化学习  Q4. 强化学习的目标是什么？ a) 最大化即时奖励 b) 最小化长期回报 c) 为智能体确定最优策略 d) 模仿环境行为  Q5. 智能体执行动作后从环境中获得什么？ a) 环境的当前状态 b) 下一动作的策略 c) 标量奖励或强化信号 d) 环境模型\"\"\"\n",
    "\n",
    "reference_answer = \"\"\"该文档是 BMCS2114 机器学习 课程的 第9次测验（Quiz 9），内容主要围绕 强化学习（Reinforcement Learning, RL） 的核心概念展开。测验包括五道多项选择题，主要考察以下知识点：  强化学习的特点，强调其是一个 序列决策过程。  反馈的性质，指出其是 延迟的，并取决于智能体的行为。  是否使用模型的问题，指出 无模型强化学习（Model-free RL） 不依赖环境模型。  强化学习的目标，是 确定最优策略（optimal policy）。  智能体与环境的交互，智能体在执行动作后会获得一个 标量奖励（scalar reward）。  需要我把整份测验逐题翻译成中文吗？\"\"\"\n",
    "\n",
    "# Compute cosine similarity\n",
    "emb1 = model.encode(chatbot_answer, convert_to_tensor=True)\n",
    "emb2 = model.encode(reference_answer, convert_to_tensor=True)\n",
    "cosine_similarity = util.cos_sim(emb1, emb2).item()\n",
    "print(f\"Cosine Similarity: {cosine_similarity:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
