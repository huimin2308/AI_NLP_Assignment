{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1fbe682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Install necessary libraries\n",
    "# !pip install nltk docx2txt pdfreader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce3ff40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK resources...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK is working\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed Tokens: ['example', 'testing', 'punctuation', 'number', 'stopwords']\n",
      "\n",
      "Testing with FakeFile for TXT:\n",
      "Raw Text from FakeFile (first 500 chars): 3. Training (train_pg)\n",
      "We run for multiple episodes (student learning sessions).\n",
      "\n",
      "In each episode:\n",
      "\n",
      "The model samples actions based on the policy’s probabilities.\n",
      "\n",
      "Executes the action, gets reward and next state from the environment.\n",
      "\n",
      "Saves log-probabilities and rewards for learning later.\n",
      "\n",
      "\n",
      "After each episode:\n",
      "\n",
      "Policy gradient loss is calculated using REINFORCE algorithm:\n",
      "\n",
      "Rewards are used to encourage good actions.\n",
      "\n",
      "An entropy term is added to encourage exploration (not alway\n",
      "Processed Tokens (first 50): ['training', 'trainpg', 'run', 'multiple', 'episode', 'student', 'learning', 'session', 'episode', 'model', 'sample', 'action', 'based', 'policy', 'probability', 'executes', 'action', 'get', 'reward', 'next', 'state', 'environment', 'save', 'logprobabilities', 'reward', 'learning', 'later', 'episode', 'policy', 'gradient', 'loss', 'calculated', 'using', 'reinforce', 'algorithm', 'reward', 'used', 'encourage', 'good', 'action', 'entropy', 'term', 'added', 'encourage', 'exploration', 'always', 'picking', 'action', 'network', 'updated']\n",
      "\n",
      "Testing with create_file_object for TXT:\n",
      "Raw Text from create_file_object (first 500 chars): 3. Training (train_pg)\n",
      "We run for multiple episodes (student learning sessions).\n",
      "\n",
      "In each episode:\n",
      "\n",
      "The model samples actions based on the policy’s probabilities.\n",
      "\n",
      "Executes the action, gets reward and next state from the environment.\n",
      "\n",
      "Saves log-probabilities and rewards for learning later.\n",
      "\n",
      "\n",
      "After each episode:\n",
      "\n",
      "Policy gradient loss is calculated using REINFORCE algorithm:\n",
      "\n",
      "Rewards are used to encourage good actions.\n",
      "\n",
      "An entropy term is added to encourage exploration (not alway\n",
      "Processed Tokens (first 50): ['training', 'trainpg', 'run', 'multiple', 'episode', 'student', 'learning', 'session', 'episode', 'model', 'sample', 'action', 'based', 'policy', 'probability', 'executes', 'action', 'get', 'reward', 'next', 'state', 'environment', 'save', 'logprobabilities', 'reward', 'learning', 'later', 'episode', 'policy', 'gradient', 'loss', 'calculated', 'using', 'reinforce', 'algorithm', 'reward', 'used', 'encourage', 'good', 'action', 'entropy', 'term', 'added', 'encourage', 'exploration', 'always', 'picking', 'action', 'network', 'updated']\n",
      "\n",
      "Testing with create_file_object for PDF:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Text from create_file_object (first 500 chars): Introduction to\n",
      "Machine Learning\n",
      "1\n",
      "Objectives\n",
      "Understand machine learning\n",
      "•\n",
      "Identify types of machine learning\n",
      "•\n",
      "Explain terminology that are related to machine learning\n",
      "•\n",
      "Understand SAS Viya for machine learning\n",
      "•\n",
      "2\n",
      "What is Machine Learning?\n",
      "Machine learning allows computers to learn\n",
      "and infer from data.\n",
      "Machine Learning gives computers the ability\n",
      "to learn without being explicitly programmed.\n",
      "3\n",
      "Types of Machine Learning Algorithms\n",
      "Supervised learning Unsupervised Learning Semi-supervised Learn\n",
      "Processed Tokens (first 50): ['introduction', 'machine', 'learning', 'objective', 'understand', 'machine', 'learning', 'identify', 'type', 'machine', 'learning', 'explain', 'terminology', 'related', 'machine', 'learning', 'understand', 'sa', 'viya', 'machine', 'learning', 'machine', 'learning', 'machine', 'learning', 'allows', 'computer', 'learn', 'infer', 'data', 'machine', 'learning', 'give', 'computer', 'ability', 'learn', 'without', 'explicitly', 'programmed', 'type', 'machine', 'learning', 'algorithm', 'supervised', 'learning', 'unsupervised', 'learning', 'semisupervised', 'learning', 'reinforcement']\n"
     ]
    }
   ],
   "source": [
    "# 1. Import your preprocessing and extraction functions\n",
    "from text_preprocessing import preprocess_text, download_nltk_resources\n",
    "from extractfile_utils import extract_file, create_file_object\n",
    "\n",
    "# 2. Download NLTK resources\n",
    "print(\"Downloading NLTK resources...\")\n",
    "download_nltk_resources()\n",
    "\n",
    "# 3. Sample text preprocessing test\n",
    "sample_text = \"This is an example! 123 Testing... with punctuation, numbers, and some stopwords.\"\n",
    "tokens = preprocess_text(sample_text)\n",
    "print(\"\\nProcessed Tokens:\", tokens)\n",
    "\n",
    "# 4. Test with local file (TXT)\n",
    "file_path_txt = \"PolicyGradientExplanation.txt\"  # Ensure this file exists\n",
    "\n",
    "# Option 1: Test with FakeFile (fixed version)\n",
    "print(\"\\nTesting with FakeFile for TXT:\")\n",
    "try:\n",
    "    class FakeFile:\n",
    "        def __init__(self, file_path, mime):\n",
    "            self.file = open(file_path, \"rb\")\n",
    "            self.type = mime\n",
    "    fake_file = FakeFile(file_path_txt, \"text/plain\")\n",
    "    raw_text = extract_file(fake_file)\n",
    "    print(\"Raw Text from FakeFile (first 500 chars):\", raw_text[:500])\n",
    "    tokens = preprocess_text(raw_text)\n",
    "    print(\"Processed Tokens (first 50):\", tokens[:50])\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File {file_path_txt} not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "finally:\n",
    "    fake_file.file.close()\n",
    "\n",
    "# Option 2: Test with create_file_object (simpler)\n",
    "print(\"\\nTesting with create_file_object for TXT:\")\n",
    "try:\n",
    "    with create_file_object(file_path_txt) as file_obj:\n",
    "        raw_text = extract_file(file_obj)\n",
    "        print(\"Raw Text from create_file_object (first 500 chars):\", raw_text[:500])\n",
    "        tokens = preprocess_text(raw_text)\n",
    "        print(\"Processed Tokens (first 50):\", tokens[:50])\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File {file_path_txt} not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "\n",
    "# 5. Test with local file (PDF)\n",
    "file_path_pdf = \"L01 Introduction to ML.pdf\"  # Ensure this file exists\n",
    "print(\"\\nTesting with create_file_object for PDF:\")\n",
    "try:\n",
    "    with create_file_object(file_path_pdf) as file_obj:\n",
    "        raw_text = extract_file(file_obj)\n",
    "        print(\"Raw Text from create_file_object (first 500 chars):\", raw_text[:500])\n",
    "        tokens = preprocess_text(raw_text)\n",
    "        print(\"Processed Tokens (first 50):\", tokens[:50])\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File {file_path_pdf} not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786679f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
